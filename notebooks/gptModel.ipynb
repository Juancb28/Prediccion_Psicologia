{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea273b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5586bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "CARPETA_TXT = \"Convert_Books\"\n",
    "CARPETA_CHUNKS = \"Chunks_Dataset\"\n",
    "CHUNK_SIZE = 1800\n",
    "OVERLAP = 300\n",
    "BLOCK_SIZE = 1_000_000  # leer 1 MB a la vez; ajústalo según RAM\n",
    "\n",
    "def dividir_en_chunks_from_file(path, size=CHUNK_SIZE, overlap=OVERLAP, block_size=BLOCK_SIZE):\n",
    "    \"\"\"\n",
    "    Generador que lee el archivo por bloques y produce (index, chunk) sin cargar todo el archivo.\n",
    "    \"\"\"\n",
    "    buffer = \"\"\n",
    "    idx = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            parte = f.read(block_size)\n",
    "            if not parte:\n",
    "                break\n",
    "            buffer += parte\n",
    "\n",
    "            # mientras haya suficiente texto en buffer, extraer chunks\n",
    "            while len(buffer) >= size:\n",
    "                ventana = buffer[:size]\n",
    "\n",
    "                corte = max(\n",
    "                    ventana.rfind(\".\"),\n",
    "                    ventana.rfind(\"?\"),\n",
    "                    ventana.rfind(\"!\"),\n",
    "                    ventana.rfind(\"\\n\")\n",
    "                )\n",
    "\n",
    "                if corte != -1 and corte > size * 0.4:\n",
    "                    len_chunk = corte + 1\n",
    "                else:\n",
    "                    len_chunk = size\n",
    "\n",
    "                chunk = buffer[:len_chunk].strip()\n",
    "                if chunk:\n",
    "                    yield idx, chunk\n",
    "                    idx += 1\n",
    "\n",
    "                # mantener solapamiento en buffer\n",
    "                buffer = buffer[len_chunk - overlap:]\n",
    "\n",
    "    # procesar resto del buffer final\n",
    "    # podemos seguir cortando hasta que quede muy pequeño\n",
    "    while buffer:\n",
    "        if len(buffer) <= 0:\n",
    "            break\n",
    "        ventana = buffer\n",
    "        corte = max(\n",
    "            ventana.rfind(\".\"),\n",
    "            ventana.rfind(\"?\"),\n",
    "            ventana.rfind(\"!\"),\n",
    "            ventana.rfind(\"\\n\")\n",
    "        )\n",
    "        if corte != -1 and corte > len(buffer) * 0.4:\n",
    "            len_chunk = corte + 1\n",
    "        else:\n",
    "            # si es pequeño, tomar todo\n",
    "            len_chunk = len(buffer)\n",
    "\n",
    "        chunk = buffer[:len_chunk].strip()\n",
    "        if chunk:\n",
    "            yield idx, chunk\n",
    "            idx += 1\n",
    "\n",
    "        buffer = buffer[len_chunk - overlap:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9065dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_chunks_stream():\n",
    "    carpeta_origen = Path(CARPETA_TXT)\n",
    "    carpeta_destino = Path(CARPETA_CHUNKS)\n",
    "    carpeta_destino.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for archivo in carpeta_origen.glob(\"*.txt\"):\n",
    "        print(\"Procesando:\", archivo.name)\n",
    "        carpeta_libro = carpeta_destino / archivo.stem\n",
    "        carpeta_libro.mkdir(exist_ok=True)\n",
    "\n",
    "        total = 0\n",
    "        for i, chunk in dividir_en_chunks_from_file(archivo):\n",
    "            data = {\n",
    "                \"id\": f\"{archivo.stem}_chunk_{i}\",\n",
    "                \"book_id\": archivo.stem,\n",
    "                \"text\": chunk,\n",
    "                \"chunk_index\": i,\n",
    "                \"char_count\": len(chunk),\n",
    "                \"word_count\": len(chunk.split())\n",
    "            }\n",
    "\n",
    "            ruta = carpeta_libro / f\"{archivo.stem}_chunk_{i}.json\"\n",
    "            with open(ruta, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                json.dump(data, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            total += 1\n",
    "            if total % 100 == 0:\n",
    "                # mensaje de progreso discreto\n",
    "                print(f\"  → {total} chunks escritos...\")\n",
    "\n",
    "        print(f\"✓ {total} chunks generados para {archivo.name}\\n\")\n",
    "\n",
    "        # liberar memoria\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ef840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de ejecutar, asegúrate de haber cerrado salidas previas y reiniciado kernel si hubo OOM.\n",
    "generar_chunks_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta de la carpeta donde guardaste los chunks\n",
    "carpeta = Path(\"Chunks_Dataset\")\n",
    "\n",
    "# Lista para almacenar todos los datos\n",
    "registros = []\n",
    "\n",
    "# Cargar todos los archivos JSON\n",
    "for archivo_json in carpeta.rglob(\"*.json\"):\n",
    "    with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        registros.append(data)\n",
    "\n",
    "# Verificar cuántos registros hemos cargado\n",
    "len(registros), registros[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1616974",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(registros)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def tokenizar(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Tokenizar todos los registros\n",
    "tokenized = dataset.map(tokenizar, batched=True)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "tokenized = tokenized.remove_columns([col for col in tokenized.column_names if col != \"input_ids\"])\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e846e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"h.\" in name:\n",
    "        capa = int(name.split(\".\")[1])\n",
    "        if capa < 6:   # Congelamos capas 0–5\n",
    "            param.requires_grad = False\n",
    "\n",
    "print(\"Capas 0–5 congeladas. Solo las capas 6–11 serán entrenadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"modelo_final\",          # carpeta donde guardaremos el modelo entrenado\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,                 # Puedes aumentar las épocas si lo deseas\n",
    "    per_device_train_batch_size=2,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f031aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    eval_dataset=tokenized\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa24f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir modelo_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_psicologia():\n",
    "    print(\"=== Chat Psicología (Escribe SALIR para terminar) ===\\n\")\n",
    "\n",
    "    while True:\n",
    "        pregunta = input(\"Tú: \")\n",
    "\n",
    "        if pregunta.lower().strip() == \"salir\":\n",
    "            print(\"Fin del chat.\")\n",
    "            break\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Actúa como profesor de psicología experto.\n",
    "Responde únicamente usando tu entrenamiento en los libros.\n",
    "\n",
    "Pregunta:\n",
    "{pregunta}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=250,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        print(\"\\nModelo:\", tokenizer.decode(output[0], skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "chat_psicologia()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
