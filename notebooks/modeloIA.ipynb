{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1764039392847,
     "user": {
      "displayName": "Institucional RM",
      "userId": "12199934497270130383"
     },
     "user_tz": 300
    },
    "id": "EXYNBWl-5AIb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(102,\n",
       " {'id': '1_Biological_Psychology_chunk_0',\n",
       "  'book_id': '1_Biological_Psychology',\n",
       "  'source_file': 'Biological Psychology.txt',\n",
       "  'source_path': 'Convert_Books\\\\TestBookClean\\\\Biological Psychology.txt',\n",
       "  'chunk_index': 0,\n",
       "  'text': 'It is often said that Man is unique among animals. It is worth looking at this term unique before we discuss our subject proper. The word may in this context have two slightly different meanings. It may mean: Man is strikingly different - he is not identical with any animal. This is of course true. It is true also of all other animals: Each species, even each individual is unique in this sense. But the term is also often used in a more absolute sense: Man is so different, so \"essentially different\" (whatever that means) that the gap between him and animals cannot possibly be bridged - he is something altogether new. \\nUsed in this absolute sense the term is scientifically meaningless. Its use also reveals and may reinforce conceit, and it leads to complacency and defeatism because it assumes that it will be futile even to search for animal roots. It is prejudging the issue.\\n\\nModule 1\\n\\nModule 1.1 The Biological Approach to Behavior \\n\\nSo how did that happen? Furthermore, given the existence of a universe, why this particular type of universe? As Steven Vigdor noted, \"Our universe is habitable. It did not have to be so.\" The Big Bang that started the universe contained an as-yet unexplained asymmetry that caused the development of more matter than antimatter. If matter and antimatter had been equal, there could be no stars, planets, or anything else. Our universe has protons, neutrons, and electrons with particular imensions of mass and charge. It has four fundamental forces: gravity, electromagnetism, the strong nuclear force, and the weak nuclear force.',\n",
       "  'meta': {'char_count': 1577, 'word_count': 265}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta de la carpeta donde guardaste los chunks generados previamente\n",
    "carpeta = Path(\"Chunks_Dataset\")\n",
    "\n",
    "# Lista para almacenar todos los datos\n",
    "registros = []\n",
    "\n",
    "# Cargar todos los archivos JSON que contienen los chunks\n",
    "for archivo_json in carpeta.rglob(\"*.json\"):\n",
    "    with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        registros.append(data)\n",
    "\n",
    "# Verificar cuántos registros hemos cargado\n",
    "len(registros), registros[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4hSn0T95Bf6",
    "outputId": "6c13bff6-ba45-40c3-db6d-a42a1aeef29f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'book_id', 'source_file', 'source_path', 'chunk_index', 'text', 'meta'],\n",
       "    num_rows: 102\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_list(registros)\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FCG13Bty5DuH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 102/102 [00:00<00:00, 229.23 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 102\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Cargar el tokenizer de GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# Función para tokenizar el texto\n",
    "def tokenizar(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Tokenizar todos los registros\n",
    "tokenized = dataset.map(tokenizar, batched=True)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "tokenized = tokenized.remove_columns([col for col in tokenized.column_names if col != \"input_ids\"])\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "tokenized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "44agiAso58nl"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Cargar el modelo GPT-2 preentrenado\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vQAuvszi59I1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capas 0–5 congeladas. Solo las capas 6–11 serán entrenadas.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Congelar las primeras 6 capas (0-5) de GPT-2 si se detecta el índice\n",
    "for name, param in model.named_parameters():\n",
    "    # Buscar patrón \".h.<num>.\" en el nombre del parámetro\n",
    "    m = re.search(r'\\.h\\.(\\d+)\\.', name)\n",
    "    if not m:\n",
    "        # no corresponde a una capa 'h.<n>' (p. ej. ln_f, wte, wpe...), saltar\n",
    "        continue\n",
    "    capa = int(m.group(1))\n",
    "    if capa < 6:   # Congelamos capas 0–5\n",
    "        param.requires_grad = False\n",
    "\n",
    "print(\"Capas 0–5 congeladas. Solo las capas 6–11 serán entrenadas.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\juana\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juana\\AppData\\Local\\Temp\\ipykernel_14328\\190388764.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 00:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=153, training_loss=3.142376669092116, metrics={'train_runtime': 39.0365, 'train_samples_per_second': 7.839, 'train_steps_per_second': 3.919, 'total_flos': 79955361792000.0, 'train_loss': 3.142376669092116, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Argumentos comunes\n",
    "common_args = dict(\n",
    "    output_dir=\"./modelo_final\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Intentamos crear TrainingArguments con opciones modernas; si falla, usamos fallback mínimo\n",
    "try:\n",
    "    args = TrainingArguments(\n",
    "        **common_args,\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_eval_batch_size=2,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"no\",   # puede fallar en versiones antiguas\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        fp16=False\n",
    "    )\n",
    "except TypeError:\n",
    "    # Fallback para versiones antiguas de transformers\n",
    "    args = TrainingArguments(**common_args)\n",
    "\n",
    "# Data collator para causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Crear Trainer y entrenar\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    eval_dataset=tokenized,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import torch\n",
    "\n",
    "# seleccionar dispositivo y mover modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6Cl0R5e6FZ9"
   },
   "outputs": [],
   "source": [
    "def chat_psicologia():\n",
    "    print(\"=== Chat Psicología (Escribe SALIR para terminar) ===\\n\")\n",
    "\n",
    "    while True:\n",
    "        pregunta = input(\"Tú: \")\n",
    "\n",
    "        if pregunta.lower().strip() == \"salir\":\n",
    "            print(\"Fin del chat.\")\n",
    "            break\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "    Actúa como profesor de psicología experto.\n",
    "Responde únicamente usando tu entrenamiento en los libros.\n",
    "\n",
    "Pregunta:\n",
    "{pregunta}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=250,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        print(\"\\nModelo:\", tokenizer.decode(output[0], skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat Psicología (Escribe SALIR para terminar) ===\n",
      "\n",
      "\n",
      "Modelo: \n",
      "Actúa como profesor de psicología experto.\n",
      "Responde únicamente usando tu entrenamiento en los libros.\n",
      "\n",
      "Pregunta:\n",
      "what do dendritic spines do?\n",
      "\n",
      "Respuesta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "\n",
      "What do dendrites do?\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "Pregunta:\n",
      "\n",
      "What do dendrites do?\n",
      "\n",
      "Pregunta \n",
      "\n",
      "\n",
      "Modelo: \n",
      "Actúa como profesor de psicología experto.\n",
      "Responde únicamente usando tu entrenamiento en los libros.\n",
      "\n",
      "Pregunta:\n",
      "what do the mitochondria do? \n",
      "\n",
      "Respuesta:\n",
      "\n",
      "The mitochondria are the same thing as the cells that that we produce? \n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy. \n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as the cells that produce energy.\n",
      "\n",
      "The mitochondria are the same thing as \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_psicologia()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNsv4GMTfGHQRmc/jA9M5Ix",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
